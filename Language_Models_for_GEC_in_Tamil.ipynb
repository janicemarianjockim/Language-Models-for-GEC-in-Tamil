{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BERT"
      ],
      "metadata": {
        "id": "TzlcPWmNDbem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import BertTokenizer, EncoderDecoderModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Data Preprocessing Function\n",
        "def preprocess_data(df):\n",
        "    df = df.dropna(subset=['Error word & consecutive word', 'Corrected words & its'])\n",
        "    return df\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv('Error Annotated Corpus.csv')\n",
        "\n",
        "# Preprocess Data\n",
        "df = preprocess_data(df)\n",
        "\n",
        "# Define the Dataset\n",
        "class TamilGrammarDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        error_sentence = self.data.iloc[idx, 0]\n",
        "        corrected_sentence = self.data.iloc[idx, 1]\n",
        "\n",
        "        encoding = self.tokenizer(error_sentence, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        target_encoding = self.tokenizer(corrected_sentence, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': target_encoding['input_ids'].squeeze()\n",
        "        }\n",
        "\n",
        "# Initialize Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "dataset = TamilGrammarDataset(df, tokenizer, max_length=128)\n",
        "\n",
        "# Define a model for sequence-to-sequence tasks\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    \"bert-base-multilingual-cased\"\n",
        ")\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "def train_and_evaluate(train_index, val_index):\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
        "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_index)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=16, sampler=train_subsampler, num_workers=2)  # Increased batch size, adjusted num_workers\n",
        "    val_loader = DataLoader(dataset, batch_size=16, sampler=val_subsampler, num_workers=2)      # Increased batch size, adjusted num_workers\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "    total_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()  # Mixed precision training\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience = 3  # Early stopping patience\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(3):  # Reduce to 3 epochs for testing\n",
        "        print(f'Epoch {epoch + 1}/3')\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, scheduler, scaler)\n",
        "        val_loss, val_acc = eval_model(model, val_loader, device)\n",
        "        print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')\n",
        "        print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}')\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save the model\n",
        "            torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        # Explicit garbage collection\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return best_loss, val_acc\n",
        "\n",
        "# Training Function\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler, scaler):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():  # Mixed precision\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = outputs.logits.argmax(dim=-1)\n",
        "        correct_predictions += (preds == labels).sum().item()\n",
        "        total_predictions += labels.numel()\n",
        "\n",
        "        # Print progress every 100 batches\n",
        "        if batch_idx % 100 == 0:\n",
        "            accuracy = correct_predictions / total_predictions\n",
        "            print(f\"Batch {batch_idx}/{len(data_loader)} - Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return np.mean(losses), accuracy\n",
        "\n",
        "# Evaluation Function\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(data_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():  # Mixed precision\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = outputs.logits.argmax(dim=-1)\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_predictions += labels.numel()\n",
        "\n",
        "            # Print progress every 100 batches\n",
        "            if batch_idx % 100 == 0:\n",
        "                accuracy = correct_predictions / total_predictions\n",
        "                print(f\"Validation Batch {batch_idx}/{len(data_loader)} - Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return np.mean(losses), accuracy\n",
        "\n",
        "# Run K-Fold Cross-Validation\n",
        "best_losses = []\n",
        "best_accuracies = []\n",
        "for train_index, val_index in kf.split(dataset):\n",
        "    best_loss, best_accuracy = train_and_evaluate(train_index, val_index)\n",
        "    best_losses.append(best_loss)\n",
        "    best_accuracies.append(best_accuracy)\n",
        "\n",
        "print(f'Best losses from each fold: {best_losses}')\n",
        "print(f'Mean loss: {np.mean(best_losses)}')\n",
        "print(f'Best accuracies from each fold: {best_accuracies}')\n",
        "print(f'Mean accuracy: {np.mean(best_accuracies)}')\n"
      ],
      "metadata": {
        "id": "AFmk4nHdGgzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "517afc43455b45dda12ba648f3841f77",
            "aceab499a3fa47f1a4482e29017947b0",
            "96370f7f84b84b09a9211f7ab56eb020",
            "d7ec0dd1e7fb484ab1a03a696ea52c68",
            "52c30cab5d2442d1a67fd6e83bb58ff0",
            "7631537fb0af43a3ba5b50e07621a4d9",
            "c3e665a98d704a319f2dbc953f8298cc",
            "8b2ffb09b3ee4155ab4ad7d7cc43d4f0",
            "9fccf93cb60346549d59d520dec689c1",
            "ba46c8ff59984b4dbb1ea4c54f788b23",
            "884be320f6504470b969480ba0a61fa2",
            "6963cfba6f144582bbb7ab85e6bf6a2e",
            "bcd54b3eaeab4529b139ada8e88df8d0",
            "f5146ca3af2846e99b4a9692cced6c32",
            "4fc487c4e32841f9a49e14273f7cc7c2",
            "7dd93936f15f420286a48c0fcc06731f",
            "6865f80ac1bd4740bbe9dd7cde10d7ca",
            "5c65d35ea55b410f8a72a0c0f40b6203",
            "91218e438365458f8782f6692b806806",
            "aeda135e3d2c4246a2578eb4838a3b2a",
            "bc738419c74347c1b84d169908d16692",
            "868e4977142c4f06b4c3543ddcfd7414",
            "93bdc607a8fb4c2b87a537832c0c8a23",
            "ef59785766ed4afa8f8f3e6ea9ee1874",
            "eb785b3f56c0475d8584734dd36f7edd",
            "95e2c4a0fca14296b74bf1604f8997be",
            "b428d8b40ceb4dd095dc94cea7be7416",
            "9efff7e4e80149aca13cb720667161b6",
            "184f2ee25fc74313a9365b955df7c559",
            "c6d33ca115ae4e7f86580213413f2c6a",
            "f786059dfff5493ab71d8fcaa3817412",
            "60c0e1dc1d9f4a01a63ff17622389c75",
            "a5c58f1c28c44ed3a919f09ac883513e",
            "a1c9ce4f14fa4e678951d9d54c6cf781",
            "8ab4cf6bf182424e9708eec2090501ff",
            "fe015771f49a4afa81ca28c46c4d6194",
            "99d7c687a8634842add461ad56d36b61",
            "634d1ded7d0b46168ad8b37d2f5c0d26",
            "fbeb4ad2d1b6499c802df16972dd4e79",
            "b81e8718cf8042ac968d0e1acfbd24c7",
            "acf1b7111a2e470aa5eaf6f3b96bcd4a",
            "d8988c0b985a4500948e9aa4748ebb78",
            "c0e430a33cc94c3b97e3c9fc2b8d5241",
            "c8c4f8d0f0b94669b361749b55334f2c",
            "0812da3bba9d44d88005a6b0cf0a2d89",
            "c9cfc1eec40843fcaf27c79eab435f12",
            "a57d7c9a4d4b438b82672d5a8324c304",
            "bd2f54aa07b84cb09a49ae61b919457e",
            "b348295a5a9f44e1891afaaa92d4c1e5",
            "dea6a741cb6944598e30e8d33686b173",
            "b279d7b470af48b0856f7397a085704c",
            "11cff71ceec84c2f8fecbd53c157f89e",
            "7847dc80f03144f7a7a6925aeccb0297",
            "0b9003a65e79437d906cdad93f2b9b1e",
            "1291f764a1ab48c3b3508d5f6e1e5ac0"
          ]
        },
        "outputId": "b85bd79a-2bec-4ba9-ec6f-d61476e43196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "517afc43455b45dda12ba648f3841f77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6963cfba6f144582bbb7ab85e6bf6a2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93bdc607a8fb4c2b87a537832c0c8a23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1c9ce4f14fa4e678951d9d54c6cf781"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0812da3bba9d44d88005a6b0cf0a2d89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:623: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/251 - Loss: 23.435760498046875, Accuracy: 0.0\n",
            "Batch 100/251 - Loss: 0.6229305267333984, Accuracy: 0.7284189356435643\n",
            "Batch 200/251 - Loss: 0.4551798701286316, Accuracy: 0.8264876787935324\n",
            "Validation Batch 0/63 - Loss: 0.4332714378833771, Accuracy: 0.9326171875\n",
            "Train loss: 1.9377543494045972, Train accuracy: 0.8475259119770916\n",
            "Validation loss: 0.37566926413112217, Validation accuracy: 0.9377178784860558\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.4783584475517273, Accuracy: 0.9267578125\n",
            "Batch 100/251 - Loss: 0.4137267768383026, Accuracy: 0.9348458771658416\n",
            "Batch 200/251 - Loss: 0.38140401244163513, Accuracy: 0.9347039217972637\n",
            "Validation Batch 0/63 - Loss: 0.3505668342113495, Accuracy: 0.9375\n",
            "Train loss: 0.39918417011599144, Train accuracy: 0.9347823549551793\n",
            "Validation loss: 0.3555280708131336, Validation accuracy: 0.9387216757968128\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.36182236671447754, Accuracy: 0.935546875\n",
            "Batch 100/251 - Loss: 0.3805635869503021, Accuracy: 0.9360448251856436\n",
            "Batch 200/251 - Loss: 0.4102689325809479, Accuracy: 0.9358675373134329\n",
            "Validation Batch 0/63 - Loss: 0.3070683181285858, Accuracy: 0.94775390625\n",
            "Train loss: 0.37968732576921166, Train accuracy: 0.9358873101344621\n",
            "Validation loss: 0.34932645824220443, Validation accuracy: 0.9389628984063745\n",
            "Epoch 1/3\n",
            "Batch 0/251 - Loss: 0.42115095257759094, Accuracy: 0.92626953125\n",
            "Batch 100/251 - Loss: 0.3401113450527191, Accuracy: 0.9377852336014851\n",
            "Batch 200/251 - Loss: 0.2745680809020996, Accuracy: 0.9406215990360697\n",
            "Validation Batch 0/63 - Loss: 0.21848465502262115, Accuracy: 0.94775390625\n",
            "Train loss: 0.3029428002131413, Train accuracy: 0.941875077813745\n",
            "Validation loss: 0.24171459769445752, Validation accuracy: 0.9456004108565738\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.23072879016399384, Accuracy: 0.94970703125\n",
            "Batch 100/251 - Loss: 0.25364354252815247, Accuracy: 0.9497698793316832\n",
            "Batch 200/251 - Loss: 0.24171388149261475, Accuracy: 0.9510795631218906\n",
            "Validation Batch 0/63 - Loss: 0.19385682046413422, Accuracy: 0.9560546875\n",
            "Train loss: 0.21521471808868575, Train accuracy: 0.9520064274153387\n",
            "Validation loss: 0.19877432310391988, Validation accuracy: 0.9529304656374502\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.1570347249507904, Accuracy: 0.96044921875\n",
            "Batch 100/251 - Loss: 0.2130468338727951, Accuracy: 0.9563979347153465\n",
            "Batch 200/251 - Loss: 0.17860166728496552, Accuracy: 0.9568393384639303\n",
            "Validation Batch 0/63 - Loss: 0.16643354296684265, Accuracy: 0.962890625\n",
            "Train loss: 0.18120311996138902, Train accuracy: 0.9570721022161355\n",
            "Validation loss: 0.19020718596284353, Validation accuracy: 0.9545645542828686\n",
            "Epoch 1/3\n",
            "Batch 0/251 - Loss: 0.17134159803390503, Accuracy: 0.958984375\n",
            "Batch 100/251 - Loss: 0.1515422761440277, Accuracy: 0.9570940980816832\n",
            "Batch 200/251 - Loss: 0.15498745441436768, Accuracy: 0.9586394200870647\n",
            "Validation Batch 0/63 - Loss: 0.18200084567070007, Accuracy: 0.95166015625\n",
            "Train loss: 0.16723892481559777, Train accuracy: 0.9593228647908366\n",
            "Validation loss: 0.17380697256515895, Validation accuracy: 0.9568834038844621\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.12507952749729156, Accuracy: 0.9677734375\n",
            "Batch 100/251 - Loss: 0.12652301788330078, Accuracy: 0.9641717589727723\n",
            "Batch 200/251 - Loss: 0.14347364008426666, Accuracy: 0.9647708722014925\n",
            "Validation Batch 0/63 - Loss: 0.15543226897716522, Accuracy: 0.96240234375\n",
            "Train loss: 0.1372599010151696, Train accuracy: 0.9650363390189243\n",
            "Validation loss: 0.17376815563156492, Validation accuracy: 0.958961030876494\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.15086141228675842, Accuracy: 0.9619140625\n",
            "Batch 100/251 - Loss: 0.1198098212480545, Accuracy: 0.9677250928217822\n",
            "Batch 200/251 - Loss: 0.11853999644517899, Accuracy: 0.967831739738806\n",
            "Validation Batch 0/63 - Loss: 0.15048298239707947, Accuracy: 0.96484375\n",
            "Train loss: 0.1217540581506562, Train accuracy: 0.9680632937001992\n",
            "Validation loss: 0.16910659107897016, Validation accuracy: 0.9600582046812749\n",
            "Epoch 1/3\n",
            "Batch 0/251 - Loss: 0.1552368402481079, Accuracy: 0.96484375\n",
            "Batch 100/251 - Loss: 0.14307668805122375, Accuracy: 0.9641379176980198\n",
            "Batch 200/251 - Loss: 0.15257732570171356, Accuracy: 0.9649312033582089\n",
            "Validation Batch 0/63 - Loss: 0.13256222009658813, Accuracy: 0.9677734375\n",
            "Train loss: 0.13656138655436467, Train accuracy: 0.9651861304780877\n",
            "Validation loss: 0.10999714819684861, Validation accuracy: 0.9735199825697212\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.14752286672592163, Accuracy: 0.96435546875\n",
            "Batch 100/251 - Loss: 0.0976138561964035, Accuracy: 0.9689820544554455\n",
            "Batch 200/251 - Loss: 0.1175072118639946, Accuracy: 0.9694933535447762\n",
            "Validation Batch 0/63 - Loss: 0.09834487736225128, Accuracy: 0.9755859375\n",
            "Train loss: 0.11461939063086453, Train accuracy: 0.9695884431025896\n",
            "Validation loss: 0.10360822223481678, Validation accuracy: 0.9753875124501992\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.13728953897953033, Accuracy: 0.96044921875\n",
            "Batch 100/251 - Loss: 0.08971976488828659, Accuracy: 0.9718730662128713\n",
            "Batch 200/251 - Loss: 0.09057915955781937, Accuracy: 0.9721825443097015\n",
            "Validation Batch 0/63 - Loss: 0.11796797811985016, Accuracy: 0.97265625\n",
            "Train loss: 0.10365352178534189, Train accuracy: 0.972136843251992\n",
            "Validation loss: 0.10759839427376551, Validation accuracy: 0.9757687998007968\n",
            "Epoch 1/3\n",
            "Batch 0/251 - Loss: 0.09394548833370209, Accuracy: 0.97509765625\n",
            "Batch 100/251 - Loss: 0.11359140276908875, Accuracy: 0.9705629254331684\n",
            "Batch 200/251 - Loss: 0.10729273408651352, Accuracy: 0.9707541394589553\n",
            "Validation Batch 0/63 - Loss: 0.11258362233638763, Accuracy: 0.97265625\n",
            "Train loss: 0.1070082872156128, Train accuracy: 0.971218641060757\n",
            "Validation loss: 0.0977427689802079, Validation accuracy: 0.9779475846613546\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.07060904800891876, Accuracy: 0.98046875\n",
            "Batch 100/251 - Loss: 0.08705694228410721, Accuracy: 0.974875270730198\n",
            "Batch 200/251 - Loss: 0.09656474739313126, Accuracy: 0.9748352961753731\n",
            "Validation Batch 0/63 - Loss: 0.09910530596971512, Accuracy: 0.978515625\n",
            "Train loss: 0.09110911981577417, Train accuracy: 0.9748953405129482\n",
            "Validation loss: 0.1006735119791258, Validation accuracy: 0.9787801917330677\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.07363364845514297, Accuracy: 0.9794921875\n",
            "Batch 100/251 - Loss: 0.09815952181816101, Accuracy: 0.9773940284653465\n",
            "Batch 200/251 - Loss: 0.0593922883272171, Accuracy: 0.9773738728233831\n",
            "Validation Batch 0/63 - Loss: 0.10724219679832458, Accuracy: 0.97900390625\n",
            "Train loss: 0.08021732712112575, Train accuracy: 0.9775448985308764\n",
            "Validation loss: 0.10075067279357759, Validation accuracy: 0.9792392928286853\n",
            "Best losses from each fold: [0.34932645824220443, 0.19020718596284353, 0.16910659107897016, 0.10360822223481678, 0.0977427689802079]\n",
            "Mean loss: 0.18199824529980857\n",
            "Best accuracies from each fold: [0.9389628984063745, 0.9545645542828686, 0.9600582046812749, 0.9757687998007968, 0.9792392928286853]\n",
            "Mean accuracy: 0.96171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TransformerXL"
      ],
      "metadata": {
        "id": "swajCSsKEJbV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF6pQsgYDanH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZN9CgAwGrN8",
        "outputId": "f9361bd6-9e59-424f-d860-c2addc3ba5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch] accelerate -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0MhanFXDGGGd",
        "outputId": "78734a79-65e5-4385-ff4b-a401b69880c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.2\n",
            "    Uninstalling transformers-4.40.2:\n",
            "      Successfully uninstalled transformers-4.40.2\n",
            "Successfully installed accelerate-0.30.1 huggingface-hub-0.23.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 transformers-4.41.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "transformers"
                ]
              },
              "id": "82ca1a5ecf1747ada8aa175d66c736e0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Error Annotated Corpus.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['Error word & consecutive word', 'Corrected words & its'], inplace=True)\n",
        "\n",
        "# Ensure the columns are strings\n",
        "df['Error word & consecutive word'] = df['Error word & consecutive word'].astype(str)\n",
        "df['Corrected words & its'] = df['Corrected words & its'].astype(str)\n",
        "\n",
        "# Use a smaller subset of the dataset\n",
        "df = df.sample(frac=0.1, random_state=42)  # Use 10% of the dataset for faster training\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Datasets format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "# Data collator for sequence-to-sequence tasks\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model='t5-small'\n",
        ")\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples['Error word & consecutive word'], max_length=128, truncation=True, padding='max_length')\n",
        "    targets = tokenizer(examples['Corrected words & its'], max_length=128, truncation=True, padding='max_length')\n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    return inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Remove columns not needed for the model\n",
        "train_dataset = train_dataset.remove_columns(['Error word & consecutive word', 'Corrected words & its', 'Annotation'])\n",
        "val_dataset = val_dataset.remove_columns(['Error word & consecutive word', 'Corrected words & its', 'Annotation'])\n",
        "\n",
        "# Set the format for PyTorch\n",
        "train_dataset.set_format('torch')\n",
        "val_dataset.set_format('torch')\n",
        "\n",
        "# Load the model\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# Define a function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "\n",
        "    logits = torch.tensor(logits)  # Convert logits to a tensor\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Decode the predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate token-level accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        pred_tokens = tokenizer(pred, truncation=True, padding='max_length', max_length=128)['input_ids']\n",
        "        label_tokens = tokenizer(label, truncation=True, padding='max_length', max_length=128)['input_ids']\n",
        "        for p, l in zip(pred_tokens, label_tokens):\n",
        "            if l != tokenizer.pad_token_id:  # Ignore padding tokens\n",
        "                if p == l:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    accuracy = correct / total if total != 0 else 0\n",
        "    return {'accuracy': accuracy}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,  # Reduce the number of epochs\n",
        "    per_device_train_batch_size=8,  # Increase batch size if possible\n",
        "    per_device_eval_batch_size=8,\n",
        "    save_steps=1_000,\n",
        "    save_total_limit=1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True  # Enable mixed precision training\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280,
          "referenced_widgets": [
            "cbbb53a081d14817ae21e6ba93834829",
            "894a7d6f5f1c49cf98c8f6e6226f25af",
            "97af7595a2204bb9a9320ea2fca51955",
            "30f85ab0cf93401ebe24a6787728baba",
            "89ba079e69464c428dc3eacb56c1ab07",
            "1bb6674a2547433e8ee83d1d4b28f50b",
            "3753cd989986440ebb741ad4b7190dee",
            "0a03f52216fb476a81cbe085eee9b12d",
            "74bd3035c3de4985844ef5eb60f9b55b",
            "2b2ce1474b254997bb54f7d67ac59af5",
            "52085cbc67ac49b583b64b62f1cd8602",
            "1829ee6c11ed4f089c93b7520b2dc42c",
            "a8043a28b6354551a7868fe4f47ebbb0",
            "bb67900abbde42b3a6bc0d312de6d206",
            "9e0db4871bdb4ca2a481d8950b91d0ff",
            "9b015abd93cc43f48e60bea9f2d8640a",
            "288d76bcc19e461a80153ecbdd7ba154",
            "87f4edd79fb2475b8503a600ec49198f",
            "2800dd9aa9a6415bb07537c17470c69f",
            "d03dc6a89571441cb00f657418c949dd",
            "6e5e4b50c4344378bda48f66ef481517",
            "9c227096609741b39e1fdaac9e47a678"
          ]
        },
        "id": "hJnkhZI3x2rD",
        "outputId": "3e19e893-4935-4b86-a304-0d80aad08380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/451 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbbb53a081d14817ae21e6ba93834829"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1829ee6c11ed4f089c93b7520b2dc42c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [57/57 09:03, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.148452</td>\n",
              "      <td>0.943396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:19]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.14845219254493713, 'eval_accuracy': 0.9433962264150944, 'eval_runtime': 23.5816, 'eval_samples_per_second': 2.163, 'eval_steps_per_second': 0.297, 'epoch': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mBART"
      ],
      "metadata": {
        "id": "1RMv34BoF0KH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gey9X51Ri6y5",
        "outputId": "423c0d0c-367a-4d40-804c-a49c5357d95b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"MRNH/mbart-english-grammar-corrector\", src_lang=\"en_XX\", tgt_lang=\"en_XX\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"MRNH/mbart-english-grammar-corrector\")\n",
        "\n",
        "# Define a function to calculate accuracy\n",
        "def calculate_accuracy(dataset):\n",
        "    num_correct = 0\n",
        "    total_samples = len(dataset)\n",
        "\n",
        "    for input_sentence, target_sentence in dataset:\n",
        "        # Prepare input\n",
        "        inputs = tokenizer(input_sentence, text_target=target_sentence, return_tensors=\"pt\")\n",
        "\n",
        "        # Perform grammatical error correction\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs)\n",
        "\n",
        "        # Decode the corrected output\n",
        "        corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Check if the corrected sentence matches the target sentence\n",
        "        if corrected_sentence == target_sentence:\n",
        "            num_correct += 1\n",
        "\n",
        "    accuracy = num_correct / total_samples\n",
        "    return accuracy\n",
        "\n",
        "# Example dataset\n",
        "test_dataset = [\n",
        "    (\"I was here yesterday to studying\", \"I was here yesterday to study\"),\n",
        "    (\"He am a engineer.\", \"He is an engineer.\"),\n",
        "    (\"I goed to the store.\", \"I went to the store.\"),\n",
        "    # Add more samples as needed\n",
        "]\n",
        "\n",
        "accuracy = calculate_accuracy(test_dataset)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "lNS9RSmNNvTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Error Annotated Corpus.csv')\n",
        "\n",
        "# Remove rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "df['Error word & consecutive word'] = df['Error word & consecutive word'].apply(preprocess_text)\n",
        "df['Corrected words & its'] = df['Corrected words & its'].apply(preprocess_text)\n",
        "\n",
        "# Tokenize the sentences\n",
        "all_sentences = df['Error word & consecutive word'].tolist() + df['Corrected words & its'].tolist()\n",
        "all_words = [word for sentence in all_sentences for word in sentence.split()]\n",
        "\n",
        "# Create word-to-index and index-to-word mappings\n",
        "word_counts = Counter(all_words)\n",
        "sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "word_to_index = {word: idx + 1 for idx, word in enumerate(sorted_words)}  # Reserve index 0 for padding\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['Annotation'] = label_encoder.fit_transform(df['Annotation'])\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
        "    df['Error word & consecutive word'].tolist(),\n",
        "    df['Annotation'].tolist(),\n",
        "    test_size=0.1,  # Adjust as needed\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_index):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_index = word_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert words to indices\n",
        "        indexed_sentence = [self.word_to_index[word] for word in sentence.split()]\n",
        "\n",
        "        return {\n",
        "            'indexed_sentence': indexed_sentence,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "# Create datasets and dataloaders with padding\n",
        "train_dataset = CustomDataset(train_sentences, train_labels, word_to_index)\n",
        "val_dataset = CustomDataset(val_sentences, val_labels, word_to_index)\n",
        "\n",
        "# Pad sequences\n",
        "train_collate_fn = lambda batch: {\n",
        "    'indexed_sentence': torch.nn.utils.rnn.pad_sequence([torch.tensor(item['indexed_sentence']) for item in batch], batch_first=True),\n",
        "    'label': torch.tensor([item['label'] for item in batch])\n",
        "}\n",
        "\n",
        "val_collate_fn = lambda batch: {\n",
        "    'indexed_sentence': torch.nn.utils.rnn.pad_sequence([torch.tensor(item['indexed_sentence']) for item in batch], batch_first=True),\n",
        "    'label': torch.tensor([item['label'] for item in batch])\n",
        "}\n",
        "\n",
        "# Define the DataLoader with padding\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=train_collate_fn)  # Increased batch size\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, collate_fn=val_collate_fn)  # Increased batch size\n",
        "\n",
        "# Define the BiLSTM model with dropout and regularization\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.5):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.dropout(embedded)  # Applying dropout to the embeddings\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = torch.relu(self.fc1(output[:, -1, :]))  # Use ReLU activation function for the first fully connected layer\n",
        "        out = self.fc2(output)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model with dropout\n",
        "vocab_size = len(word_to_index) + 1  # Add 1 for padding\n",
        "embedding_dim = 128  # Adjusted embedding dimension\n",
        "hidden_dim = 256  # Adjusted hidden dimension\n",
        "output_dim = len(label_encoder.classes_)\n",
        "dropout = 0.4  # Adjusted dropout rate\n",
        "model = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, dropout=dropout)\n",
        "\n",
        "# Move model to appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Added L2 regularization with weight decay\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30  # Increased number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        inputs, labels = batch['indexed_sentence'].to(device), batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            inputs, labels = batch['indexed_sentence'].to(device), batch['label'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct_preds / total_preds\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss / len(train_dataloader):.4f}, Val Loss: {val_loss / len(val_dataloader):.4f}, Val Acc: {val_accuracy:.2%}\")\n",
        "\n",
        "print('Training finished.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_MxInjY6W_a",
        "outputId": "06b8531e-3de3-4e51-db02-7c3e3a22ad69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Epoch 1/30: 100%|██████████| 71/71 [00:03<00:00, 22.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train Loss: 2.4783, Val Loss: 2.2007, Val Acc: 25.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 71/71 [00:03<00:00, 18.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/30, Train Loss: 2.1468, Val Loss: 2.1946, Val Acc: 30.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 71/71 [00:03<00:00, 22.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/30, Train Loss: 2.0322, Val Loss: 2.0900, Val Acc: 34.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 71/71 [00:03<00:00, 22.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/30, Train Loss: 1.8974, Val Loss: 2.0208, Val Acc: 36.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 71/71 [00:03<00:00, 20.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/30, Train Loss: 1.7821, Val Loss: 1.9217, Val Acc: 40.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|██████████| 71/71 [00:03<00:00, 20.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/30, Train Loss: 1.6330, Val Loss: 1.8602, Val Acc: 40.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|██████████| 71/71 [00:03<00:00, 21.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/30, Train Loss: 1.5018, Val Loss: 1.8399, Val Acc: 43.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|██████████| 71/71 [00:03<00:00, 22.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/30, Train Loss: 1.3750, Val Loss: 1.8284, Val Acc: 42.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|██████████| 71/71 [00:03<00:00, 18.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/30, Train Loss: 1.2337, Val Loss: 1.8349, Val Acc: 44.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|██████████| 71/71 [00:03<00:00, 22.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/30, Train Loss: 1.0949, Val Loss: 1.7993, Val Acc: 48.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|██████████| 71/71 [00:03<00:00, 22.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/30, Train Loss: 0.9632, Val Loss: 1.8873, Val Acc: 47.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|██████████| 71/71 [00:03<00:00, 21.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/30, Train Loss: 0.8789, Val Loss: 1.8581, Val Acc: 51.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|██████████| 71/71 [00:03<00:00, 19.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/30, Train Loss: 0.7687, Val Loss: 2.1101, Val Acc: 50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|██████████| 71/71 [00:03<00:00, 21.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/30, Train Loss: 0.6852, Val Loss: 2.0493, Val Acc: 51.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30: 100%|██████████| 71/71 [00:03<00:00, 19.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/30, Train Loss: 0.5855, Val Loss: 2.1189, Val Acc: 51.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30: 100%|██████████| 71/71 [00:03<00:00, 18.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/30, Train Loss: 0.5340, Val Loss: 2.2054, Val Acc: 51.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30: 100%|██████████| 71/71 [00:03<00:00, 18.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/30, Train Loss: 0.4772, Val Loss: 2.2144, Val Acc: 52.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30: 100%|██████████| 71/71 [00:04<00:00, 14.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/30, Train Loss: 0.4164, Val Loss: 2.4902, Val Acc: 52.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30: 100%|██████████| 71/71 [00:06<00:00, 10.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/30, Train Loss: 0.3613, Val Loss: 2.6054, Val Acc: 53.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30: 100%|██████████| 71/71 [00:09<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/30, Train Loss: 0.3457, Val Loss: 2.5204, Val Acc: 54.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30: 100%|██████████| 71/71 [00:12<00:00,  5.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/30, Train Loss: 0.3022, Val Loss: 2.6184, Val Acc: 53.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30: 100%|██████████| 71/71 [00:18<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/30, Train Loss: 0.2845, Val Loss: 2.6524, Val Acc: 56.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30: 100%|██████████| 71/71 [00:24<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/30, Train Loss: 0.2589, Val Loss: 3.0733, Val Acc: 54.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30: 100%|██████████| 71/71 [00:25<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/30, Train Loss: 0.2536, Val Loss: 3.1325, Val Acc: 54.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30: 100%|██████████| 71/71 [00:24<00:00,  2.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/30, Train Loss: 0.2084, Val Loss: 2.9564, Val Acc: 54.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/30: 100%|██████████| 71/71 [00:24<00:00,  2.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/30, Train Loss: 0.2020, Val Loss: 3.3480, Val Acc: 54.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/30: 100%|██████████| 71/71 [00:26<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/30, Train Loss: 0.2012, Val Loss: 3.4419, Val Acc: 53.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/30: 100%|██████████| 71/71 [00:26<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/30, Train Loss: 0.1778, Val Loss: 3.3694, Val Acc: 55.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/30: 100%|██████████| 71/71 [00:25<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/30, Train Loss: 0.1806, Val Loss: 3.2710, Val Acc: 56.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/30: 100%|██████████| 71/71 [00:25<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/30, Train Loss: 0.1766, Val Loss: 3.2101, Val Acc: 55.78%\n",
            "Training finished.\n"
          ]
        }
      ]
    }
  ]
}