{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BERT"
      ],
      "metadata": {
        "id": "TzlcPWmNDbem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from transformers import BertTokenizer, EncoderDecoderModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Data Preprocessing Function\n",
        "def preprocess_data(df):\n",
        "    df = df.dropna(subset=['Error word & consecutive word', 'Corrected words & its'])\n",
        "    return df\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv('Error Annotated Corpus.csv')\n",
        "\n",
        "# Preprocess Data\n",
        "df = preprocess_data(df)\n",
        "\n",
        "# Define the Dataset\n",
        "class TamilGrammarDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        error_sentence = self.data.iloc[idx, 0]\n",
        "        corrected_sentence = self.data.iloc[idx, 1]\n",
        "\n",
        "        encoding = self.tokenizer(error_sentence, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        target_encoding = self.tokenizer(corrected_sentence, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': target_encoding['input_ids'].squeeze()\n",
        "        }\n",
        "\n",
        "# Initialize Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "dataset = TamilGrammarDataset(df, tokenizer, max_length=128)\n",
        "\n",
        "# Define a model for sequence-to-sequence tasks\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    \"bert-base-multilingual-cased\"\n",
        ")\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "\n",
        "# Ensure device is defined\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "def train_and_evaluate(train_index, val_index):\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
        "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_index)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=16, sampler=train_subsampler, num_workers=2)  # Increased batch size, adjusted num_workers\n",
        "    val_loader = DataLoader(dataset, batch_size=16, sampler=val_subsampler, num_workers=2)      # Increased batch size, adjusted num_workers\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "    total_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()  # Mixed precision training\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience = 3  # Early stopping patience\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(3):\n",
        "        print(f'Epoch {epoch + 1}/3')\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, scheduler, scaler)\n",
        "        val_loss, val_acc = eval_model(model, val_loader, device)\n",
        "        print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')\n",
        "        print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}')\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save the model\n",
        "            torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        # Explicit garbage collection\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return best_loss, val_acc\n",
        "\n",
        "# Training Function\n",
        "def train_epoch(model, data_loader, optimizer, device, scheduler, scaler):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():  # Mixed precision\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = outputs.logits.argmax(dim=-1)\n",
        "        correct_predictions += (preds == labels).sum().item()\n",
        "        total_predictions += labels.numel()\n",
        "\n",
        "        # Print progress every 100 batches\n",
        "        if batch_idx % 100 == 0:\n",
        "            accuracy = correct_predictions / total_predictions\n",
        "            print(f\"Batch {batch_idx}/{len(data_loader)} - Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return np.mean(losses), accuracy\n",
        "\n",
        "# Evaluation Function\n",
        "def eval_model(model, data_loader, device):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(data_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():  # Mixed precision\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = outputs.logits.argmax(dim=-1)\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_predictions += labels.numel()\n",
        "\n",
        "            # Print progress every 100 batches\n",
        "            if batch_idx % 100 == 0:\n",
        "                accuracy = correct_predictions / total_predictions\n",
        "                print(f\"Validation Batch {batch_idx}/{len(data_loader)} - Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return np.mean(losses), accuracy\n",
        "\n",
        "# Run K-Fold Cross-Validation\n",
        "best_losses = []\n",
        "best_accuracies = []\n",
        "for train_index, val_index in kf.split(dataset):\n",
        "    best_loss, best_accuracy = train_and_evaluate(train_index, val_index)\n",
        "    best_losses.append(best_loss)\n",
        "    best_accuracies.append(best_accuracy)\n",
        "\n",
        "print(f'Best losses from each fold: {best_losses}')\n",
        "print(f'Mean loss: {np.mean(best_losses)}')\n",
        "print(f'Best accuracies from each fold: {best_accuracies}')\n",
        "print(f'Mean accuracy: {np.mean(best_accuracies)}')\n"
      ],
      "metadata": {
        "id": "AFmk4nHdGgzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "517afc43455b45dda12ba648f3841f77",
            "aceab499a3fa47f1a4482e29017947b0",
            "96370f7f84b84b09a9211f7ab56eb020",
            "d7ec0dd1e7fb484ab1a03a696ea52c68",
            "52c30cab5d2442d1a67fd6e83bb58ff0",
            "7631537fb0af43a3ba5b50e07621a4d9",
            "c3e665a98d704a319f2dbc953f8298cc",
            "8b2ffb09b3ee4155ab4ad7d7cc43d4f0",
            "9fccf93cb60346549d59d520dec689c1",
            "ba46c8ff59984b4dbb1ea4c54f788b23",
            "884be320f6504470b969480ba0a61fa2",
            "6963cfba6f144582bbb7ab85e6bf6a2e",
            "bcd54b3eaeab4529b139ada8e88df8d0",
            "f5146ca3af2846e99b4a9692cced6c32",
            "4fc487c4e32841f9a49e14273f7cc7c2",
            "7dd93936f15f420286a48c0fcc06731f",
            "6865f80ac1bd4740bbe9dd7cde10d7ca",
            "5c65d35ea55b410f8a72a0c0f40b6203",
            "91218e438365458f8782f6692b806806",
            "aeda135e3d2c4246a2578eb4838a3b2a",
            "bc738419c74347c1b84d169908d16692",
            "868e4977142c4f06b4c3543ddcfd7414",
            "93bdc607a8fb4c2b87a537832c0c8a23",
            "ef59785766ed4afa8f8f3e6ea9ee1874",
            "eb785b3f56c0475d8584734dd36f7edd",
            "95e2c4a0fca14296b74bf1604f8997be",
            "b428d8b40ceb4dd095dc94cea7be7416",
            "9efff7e4e80149aca13cb720667161b6",
            "184f2ee25fc74313a9365b955df7c559",
            "c6d33ca115ae4e7f86580213413f2c6a",
            "f786059dfff5493ab71d8fcaa3817412",
            "60c0e1dc1d9f4a01a63ff17622389c75",
            "a5c58f1c28c44ed3a919f09ac883513e",
            "a1c9ce4f14fa4e678951d9d54c6cf781",
            "8ab4cf6bf182424e9708eec2090501ff",
            "fe015771f49a4afa81ca28c46c4d6194",
            "99d7c687a8634842add461ad56d36b61",
            "634d1ded7d0b46168ad8b37d2f5c0d26",
            "fbeb4ad2d1b6499c802df16972dd4e79",
            "b81e8718cf8042ac968d0e1acfbd24c7",
            "acf1b7111a2e470aa5eaf6f3b96bcd4a",
            "d8988c0b985a4500948e9aa4748ebb78",
            "c0e430a33cc94c3b97e3c9fc2b8d5241",
            "c8c4f8d0f0b94669b361749b55334f2c",
            "0812da3bba9d44d88005a6b0cf0a2d89",
            "c9cfc1eec40843fcaf27c79eab435f12",
            "a57d7c9a4d4b438b82672d5a8324c304",
            "bd2f54aa07b84cb09a49ae61b919457e",
            "b348295a5a9f44e1891afaaa92d4c1e5",
            "dea6a741cb6944598e30e8d33686b173",
            "b279d7b470af48b0856f7397a085704c",
            "11cff71ceec84c2f8fecbd53c157f89e",
            "7847dc80f03144f7a7a6925aeccb0297",
            "0b9003a65e79437d906cdad93f2b9b1e",
            "1291f764a1ab48c3b3508d5f6e1e5ac0"
          ]
        },
        "outputId": "b85bd79a-2bec-4ba9-ec6f-d61476e43196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "517afc43455b45dda12ba648f3841f77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6963cfba6f144582bbb7ab85e6bf6a2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93bdc607a8fb4c2b87a537832c0c8a23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1c9ce4f14fa4e678951d9d54c6cf781"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0812da3bba9d44d88005a6b0cf0a2d89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:623: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
            "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:643: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0/251 - Loss: 23.435760498046875, Accuracy: 0.0\n",
            "Batch 100/251 - Loss: 0.6229305267333984, Accuracy: 0.7284189356435643\n",
            "Batch 200/251 - Loss: 0.4551798701286316, Accuracy: 0.8264876787935324\n",
            "Validation Batch 0/63 - Loss: 0.4332714378833771, Accuracy: 0.9326171875\n",
            "Train loss: 1.9377543494045972, Train accuracy: 0.8475259119770916\n",
            "Validation loss: 0.37566926413112217, Validation accuracy: 0.9377178784860558\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.4783584475517273, Accuracy: 0.9267578125\n",
            "Batch 100/251 - Loss: 0.4137267768383026, Accuracy: 0.9348458771658416\n",
            "Batch 200/251 - Loss: 0.38140401244163513, Accuracy: 0.9347039217972637\n",
            "Validation Batch 0/63 - Loss: 0.3505668342113495, Accuracy: 0.9375\n",
            "Train loss: 0.39918417011599144, Train accuracy: 0.9347823549551793\n",
            "Validation loss: 0.3555280708131336, Validation accuracy: 0.9387216757968128\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.36182236671447754, Accuracy: 0.935546875\n",
            "Batch 100/251 - Loss: 0.3805635869503021, Accuracy: 0.9360448251856436\n",
            "Batch 200/251 - Loss: 0.4102689325809479, Accuracy: 0.9358675373134329\n",
            "Validation Batch 0/63 - Loss: 0.3070683181285858, Accuracy: 0.94775390625\n",
            "Train loss: 0.37968732576921166, Train accuracy: 0.9358873101344621\n",
            "Validation loss: 0.34932645824220443, Validation accuracy: 0.9389628984063745\n",
            "Epoch 1/3\n",
            "Batch 0/251 - Loss: 0.42115095257759094, Accuracy: 0.92626953125\n",
            "Batch 100/251 - Loss: 0.3401113450527191, Accuracy: 0.9377852336014851\n",
            "Batch 200/251 - Loss: 0.2745680809020996, Accuracy: 0.9406215990360697\n",
            "Validation Batch 0/63 - Loss: 0.21848465502262115, Accuracy: 0.94775390625\n",
            "Train loss: 0.3029428002131413, Train accuracy: 0.941875077813745\n",
            "Validation loss: 0.24171459769445752, Validation accuracy: 0.9456004108565738\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.23072879016399384, Accuracy: 0.94970703125\n",
            "Batch 100/251 - Loss: 0.25364354252815247, Accuracy: 0.9497698793316832\n",
            "Batch 200/251 - Loss: 0.24171388149261475, Accuracy: 0.9510795631218906\n",
            "Validation Batch 0/63 - Loss: 0.19385682046413422, Accuracy: 0.9560546875\n",
            "Train loss: 0.21521471808868575, Train accuracy: 0.9520064274153387\n",
            "Validation loss: 0.19877432310391988, Validation accuracy: 0.9529304656374502\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.1570347249507904, Accuracy: 0.96044921875\n",
            "Batch 100/251 - Loss: 0.2130468338727951, Accuracy: 0.9563979347153465\n",
            "Batch 200/251 - Loss: 0.17860166728496552, Accuracy: 0.9568393384639303\n",
            "Validation Batch 0/63 - Loss: 0.16643354296684265, Accuracy: 0.962890625\n",
            "Train loss: 0.18120311996138902, Train accuracy: 0.9570721022161355\n",
            "Validation loss: 0.19020718596284353, Validation accuracy: 0.9545645542828686\n",
            "Epoch 1/3\n",
            "Batch 0/251 - Loss: 0.17134159803390503, Accuracy: 0.958984375\n",
            "Batch 100/251 - Loss: 0.1515422761440277, Accuracy: 0.9570940980816832\n",
            "Batch 200/251 - Loss: 0.15498745441436768, Accuracy: 0.9586394200870647\n",
            "Validation Batch 0/63 - Loss: 0.18200084567070007, Accuracy: 0.95166015625\n",
            "Train loss: 0.16723892481559777, Train accuracy: 0.9593228647908366\n",
            "Validation loss: 0.17380697256515895, Validation accuracy: 0.9568834038844621\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.12507952749729156, Accuracy: 0.9677734375\n",
            "Batch 100/251 - Loss: 0.12652301788330078, Accuracy: 0.9641717589727723\n",
            "Batch 200/251 - Loss: 0.14347364008426666, Accuracy: 0.9647708722014925\n",
            "Validation Batch 0/63 - Loss: 0.15543226897716522, Accuracy: 0.96240234375\n",
            "Train loss: 0.1372599010151696, Train accuracy: 0.9650363390189243\n",
            "Validation loss: 0.17376815563156492, Validation accuracy: 0.958961030876494\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.15086141228675842, Accuracy: 0.9619140625\n",
            "Batch 100/251 - Loss: 0.1198098212480545, Accuracy: 0.9677250928217822\n",
            "Batch 200/251 - Loss: 0.11853999644517899, Accuracy: 0.967831739738806\n",
            "Validation Batch 0/63 - Loss: 0.15048298239707947, Accuracy: 0.96484375\n",
            "Train loss: 0.1217540581506562, Train accuracy: 0.9680632937001992\n",
            "Validation loss: 0.16910659107897016, Validation accuracy: 0.9600582046812749\n",
            "Epoch 1/3\n",
            "Batch 0/251 - Loss: 0.1552368402481079, Accuracy: 0.96484375\n",
            "Batch 100/251 - Loss: 0.14307668805122375, Accuracy: 0.9641379176980198\n",
            "Batch 200/251 - Loss: 0.15257732570171356, Accuracy: 0.9649312033582089\n",
            "Validation Batch 0/63 - Loss: 0.13256222009658813, Accuracy: 0.9677734375\n",
            "Train loss: 0.13656138655436467, Train accuracy: 0.9651861304780877\n",
            "Validation loss: 0.10999714819684861, Validation accuracy: 0.9735199825697212\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.14752286672592163, Accuracy: 0.96435546875\n",
            "Batch 100/251 - Loss: 0.0976138561964035, Accuracy: 0.9689820544554455\n",
            "Batch 200/251 - Loss: 0.1175072118639946, Accuracy: 0.9694933535447762\n",
            "Validation Batch 0/63 - Loss: 0.09834487736225128, Accuracy: 0.9755859375\n",
            "Train loss: 0.11461939063086453, Train accuracy: 0.9695884431025896\n",
            "Validation loss: 0.10360822223481678, Validation accuracy: 0.9753875124501992\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.13728953897953033, Accuracy: 0.96044921875\n",
            "Batch 100/251 - Loss: 0.08971976488828659, Accuracy: 0.9718730662128713\n",
            "Batch 200/251 - Loss: 0.09057915955781937, Accuracy: 0.9721825443097015\n",
            "Validation Batch 0/63 - Loss: 0.11796797811985016, Accuracy: 0.97265625\n",
            "Train loss: 0.10365352178534189, Train accuracy: 0.972136843251992\n",
            "Validation loss: 0.10759839427376551, Validation accuracy: 0.9757687998007968\n",
            "Epoch 1/3\n",
            "Batch 0/251 - Loss: 0.09394548833370209, Accuracy: 0.97509765625\n",
            "Batch 100/251 - Loss: 0.11359140276908875, Accuracy: 0.9705629254331684\n",
            "Batch 200/251 - Loss: 0.10729273408651352, Accuracy: 0.9707541394589553\n",
            "Validation Batch 0/63 - Loss: 0.11258362233638763, Accuracy: 0.97265625\n",
            "Train loss: 0.1070082872156128, Train accuracy: 0.971218641060757\n",
            "Validation loss: 0.0977427689802079, Validation accuracy: 0.9779475846613546\n",
            "Epoch 2/3\n",
            "Batch 0/251 - Loss: 0.07060904800891876, Accuracy: 0.98046875\n",
            "Batch 100/251 - Loss: 0.08705694228410721, Accuracy: 0.974875270730198\n",
            "Batch 200/251 - Loss: 0.09656474739313126, Accuracy: 0.9748352961753731\n",
            "Validation Batch 0/63 - Loss: 0.09910530596971512, Accuracy: 0.978515625\n",
            "Train loss: 0.09110911981577417, Train accuracy: 0.9748953405129482\n",
            "Validation loss: 0.1006735119791258, Validation accuracy: 0.9787801917330677\n",
            "Epoch 3/3\n",
            "Batch 0/251 - Loss: 0.07363364845514297, Accuracy: 0.9794921875\n",
            "Batch 100/251 - Loss: 0.09815952181816101, Accuracy: 0.9773940284653465\n",
            "Batch 200/251 - Loss: 0.0593922883272171, Accuracy: 0.9773738728233831\n",
            "Validation Batch 0/63 - Loss: 0.10724219679832458, Accuracy: 0.97900390625\n",
            "Train loss: 0.08021732712112575, Train accuracy: 0.9775448985308764\n",
            "Validation loss: 0.10075067279357759, Validation accuracy: 0.9792392928286853\n",
            "Best losses from each fold: [0.34932645824220443, 0.19020718596284353, 0.16910659107897016, 0.10360822223481678, 0.0977427689802079]\n",
            "Mean loss: 0.18199824529980857\n",
            "Best accuracies from each fold: [0.9389628984063745, 0.9545645542828686, 0.9600582046812749, 0.9757687998007968, 0.9792392928286853]\n",
            "Mean accuracy: 0.96171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "T5"
      ],
      "metadata": {
        "id": "swajCSsKEJbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZN9CgAwGrN8",
        "outputId": "f9361bd6-9e59-424f-d860-c2addc3ba5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.4)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch] accelerate -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0MhanFXDGGGd",
        "outputId": "78734a79-65e5-4385-ff4b-a401b69880c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, transformers, accelerate\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.40.2\n",
            "    Uninstalling transformers-4.40.2:\n",
            "      Successfully uninstalled transformers-4.40.2\n",
            "Successfully installed accelerate-0.30.1 huggingface-hub-0.23.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 transformers-4.41.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub",
                  "transformers"
                ]
              },
              "id": "82ca1a5ecf1747ada8aa175d66c736e0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1225474-5db5-4b9b-ec50-cbb4d015c6a0",
        "id": "fpAw3eQgQyNE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              },
              "id": "285441b550d248a4944eaa1e8c324633"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Error Annotated Corpus.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['Error word & consecutive word', 'Corrected words & its'], inplace=True)\n",
        "\n",
        "# Ensure the columns are strings\n",
        "df['Error word & consecutive word'] = df['Error word & consecutive word'].astype(str)\n",
        "df['Corrected words & its'] = df['Corrected words & its'].astype(str)\n",
        "\n",
        "# Use a smaller subset of the dataset\n",
        "df = df.sample(frac=0.1, random_state=42)  # Use 10% of the dataset for faster training\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Datasets format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "# Data collator for sequence-to-sequence tasks\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model='t5-small'\n",
        ")\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples['Error word & consecutive word'], max_length=128, truncation=True, padding='max_length')\n",
        "    targets = tokenizer(examples['Corrected words & its'], max_length=128, truncation=True, padding='max_length')\n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    return inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Remove columns not needed for the model\n",
        "train_dataset = train_dataset.remove_columns(['Annotation'])\n",
        "val_dataset = val_dataset.remove_columns(['Annotation'])\n",
        "\n",
        "# Set the format for PyTorch\n",
        "train_dataset.set_format('torch')\n",
        "val_dataset.set_format('torch')\n",
        "\n",
        "# Load the model\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# Define a function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "\n",
        "    logits = torch.tensor(logits)  # Convert logits to a tensor\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Decode the predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate token-level accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        pred_tokens = tokenizer(pred, truncation=True, padding='max_length', max_length=128)['input_ids']\n",
        "        label_tokens = tokenizer(label, truncation=True, padding='max_length', max_length=128)['input_ids']\n",
        "        for p, l in zip(pred_tokens, label_tokens):\n",
        "            if l != tokenizer.pad_token_id:  # Ignore padding tokens\n",
        "                if p == l:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    accuracy = correct / total if total != 0 else 0\n",
        "    return {'accuracy': accuracy}\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,  # Reduce the number of epochs\n",
        "    per_device_train_batch_size=8,  # Increase batch size if possible\n",
        "    per_device_eval_batch_size=8,\n",
        "    save_steps=1_000,\n",
        "    save_total_limit=1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True  # Enable mixed precision training\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "kmRnbIDDd5UP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628,
          "referenced_widgets": [
            "f528fbb572354287aa4abfb29ed87431",
            "11c41c0cd085471d85f6dbef5699549e",
            "6f198dea4dad43ab977388641c5393e7",
            "dd815fdef9904511bc07c3817158e0bb",
            "dc70150166d54674a0f1f024f54665ba",
            "02b7330ce6d74118a338432adb8695ed",
            "4a26243e5a9a4be09f18b3134d83e145",
            "777ebfa04ae541e5a0e6fac3ccfa6019",
            "aaca74388e374900b0d7126aeab9913a",
            "e638872b5f2e4145b717a1dce643f0ca",
            "b20c9f0004f349b597c4df6e04cfd657",
            "6ede920ffd0f45c1a2e2f58f40d990e2",
            "c16dcb07f9614bca840507ec7e56fe4d",
            "4935aa8297d24560b31ae6dd4314709c",
            "d10db81c171f451eae333a27a04ea955",
            "2508424a98db4196a5f77bda52aed218",
            "effea71a434e48ee9e29e104751521a2",
            "1d15336ae23040aa99d9d76a43eaedbe",
            "f52e50aeb40842b39a92d0f85a0280a0",
            "13e3d2fbde3843d99a8321b2f04b20ca",
            "a48f9a40d2204c4a8fdc2067880b5b00",
            "8ce3be02f39f4806b64686e00256db0a",
            "790c61d8373e443eb62f6b31fbd44645",
            "e4c63c1952c74620987c4040886f5946",
            "4923e0f31b0c42c0b47f48eb4bfec723",
            "90ec06fc828f41179f14fe9c585f09fc",
            "914e4980203540f786e78b311924879e",
            "105a34d7d93e4c0c826c3cf3f98e3380",
            "3ac74d7197c945f8be68b6206da58ed9",
            "2a159f7e68374dc29e0f756e7fb2e711",
            "6bd81b64c8304cbfb4190822fe8cb9b2",
            "c2a28ef2071641bdb405175763850c8e",
            "4161ea4595de42edbb00d702f54cd5b4",
            "da78d0858d7f4fd8bce9dec0eda5a553",
            "504ccdcabd3847cba750f706f752a224",
            "5d6fabae000045f08f97776889449bfb",
            "389433d21c604c76b4fada111602e487",
            "498b10fc3bf04bf8b0002d27fe2f2056",
            "7e2a6f97533f4467880660445f16c29c",
            "3c8d5e27facf4512963131ae15c62ed6",
            "c0a18735263f456c8d715d38817b3902",
            "dbed1bf25fe447789a29add3341c0bda",
            "853005b3db91462fa51852b9a439df6a",
            "15e4baf76af142cca1a303d84a64f396",
            "02ac681ecd244febbe42b5517634a4be",
            "6148110c71df4940a23d62977e98ac71",
            "8fd9d762b1c2467d9a9ba873005224f7",
            "92d4aecce54041a4a22b0efc0406b02a",
            "edfe0b95b3e240f5baf12495468a6a8a",
            "fba07bd1af86475f86b6dff89ccbd46f",
            "0a430b15a6f04a3f831c60a0d369b10b",
            "fde0b800ddb84063844a4c0f00a0019f",
            "806dfcea88ab42119215a3292a3975bc",
            "9a4e84633bac4fb8a60d8ef1d81c36ab",
            "da68061f3a6a40bbadf4359b0d35f3b2",
            "eb83a3a41f564785943fc8539813c5cc",
            "a8f9748b377d434c9e832778ec7a34c8",
            "c63b9cde1ce6436d8e82652a59ed1a7b",
            "262e278b38354c50b498490f609fbe51",
            "78d0d088eeed43659a13c430e04ed65c",
            "ad7eb361651d4892916a2e4885864d5b",
            "ab685219bbe9492cac0b6feac5d0c22f",
            "a8f24e82b6c147f4adb96a3241ada901",
            "dae5462c0a454dddb84af344bb7a367c",
            "ac717a1aae4349a1980d25aaca4fcc57",
            "c20ed174dd10440f9e7bd9881cbcbae5",
            "73e0e32e88cd485b939a575d7890868b",
            "24ba709ceaa54b8d8855c9f600af75e7",
            "1ad869ba66e943e08f818dff533b60c1",
            "6146293b33b44233b151692c1a786558",
            "1ff7d026986d4193b869418e0e87e38f",
            "6fd3f949db854b64ac11f32de5ec334e",
            "cd54536ae5124fd5a7ae90ba187157ae",
            "46c65fe256db4caab230e2f3bf96965a",
            "9de3b208214a4f7c9c63be85fa21459c",
            "1d0a592ab4134d0094febbe31d19648e",
            "f5c74919780440f4ada307bb728f5478",
            "fcfbe3e9af924c1792edb89438558736",
            "735675d172af4bd9a45f03f4da18e01e",
            "bead9e31e242443a865a8eaa74e4d544",
            "8a4f9f5f87d84913b3adee24f2e6cea2",
            "5fbd5c419c814717ab02ea7c2e4fcbe8",
            "fdc43a2de554485aa11034fef20c5d04",
            "756b305a80294680b149c9b9255a9a5c",
            "3c362ce0664646ddb5de1a616f499c74",
            "4bd70c2ded294e078f3f7bd8d8b7602f",
            "435bee1604f549a2be5fd1ad5d24bc31",
            "513ae581290b424eb7762c6a6ad8fe03"
          ]
        },
        "outputId": "6ae22413-b7cf-47d1-acf1-f61b18047ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f528fbb572354287aa4abfb29ed87431"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ede920ffd0f45c1a2e2f58f40d990e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "790c61d8373e443eb62f6b31fbd44645"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/451 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da78d0858d7f4fd8bce9dec0eda5a553"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02ac681ecd244febbe42b5517634a4be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb83a3a41f564785943fc8539813c5cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73e0e32e88cd485b939a575d7890868b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcfbe3e9af924c1792edb89438558736"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [57/57 10:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.148452</td>\n",
              "      <td>0.943396</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:18]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.14845219254493713, 'eval_accuracy': 0.9433962264150944, 'eval_runtime': 23.2895, 'eval_samples_per_second': 2.19, 'eval_steps_per_second': 0.301, 'epoch': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mBART"
      ],
      "metadata": {
        "id": "1RMv34BoF0KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imUkuwjxTBTk",
        "outputId": "d06b4f8e-b5ac-461d-8b57-eef0414ad3fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqLszvkNfORA",
        "outputId": "8b454d57-7c7b-4531-e8b1-9a06884c5171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.32.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Load the mBART model and tokenizer\n",
        "model_name = 'facebook/mbart-large-50-many-to-many-mmt'\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"ta_IN\", tgt_lang=\"ta_IN\")\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = 'Error Annotated Corpus.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Ensure the CSV has the required columns\n",
        "if 'Error word & consecutive word' not in df.columns or 'Corrected words & its' not in df.columns:\n",
        "    raise ValueError(\"The dataset must contain 'Error word & consecutive word' and 'Corrected words & its' columns.\")\n",
        "\n",
        "# Drop rows where 'Corrected words & its' is NaN\n",
        "df = df.dropna(subset=['Corrected words & its'])\n",
        "\n",
        "\n",
        "# Define the dataset class\n",
        "class GrammarCorrectionDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source = str(self.data.iloc[index, 0])\n",
        "        target = str(self.data.iloc[index, 1])\n",
        "\n",
        "        source_encodings = tokenizer(source, truncation=True, padding='max_length', max_length=self.max_len)\n",
        "        target_encodings = tokenizer(target, truncation=True, padding='max_length', max_length=self.max_len)\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(source_encodings['input_ids']),\n",
        "            'attention_mask': torch.tensor(source_encodings['attention_mask']),\n",
        "            'labels': torch.tensor(target_encodings['input_ids'])\n",
        "        }\n",
        "\n",
        "# Prepare the dataset\n",
        "max_len = 64  # Reduced sequence length\n",
        "dataset = GrammarCorrectionDataset(df, tokenizer, max_len)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define the Trainer class\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the fine-tuned model\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"Evaluation results:\", eval_results)\n",
        "\n",
        "# Calculate and print final accuracy on the validation set\n",
        "def calculate_accuracy(dataset, tokenizer, model, batch_size=2, device='cpu'):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2, pin_memory=True)\n",
        "    num_correct = 0\n",
        "    total_samples = len(dataset)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_sentences = batch['input_ids'].to(device)\n",
        "            attention_masks = batch['attention_mask'].to(device)\n",
        "            target_sentences = batch['labels'].to(device)\n",
        "\n",
        "            # Perform grammatical error correction\n",
        "            outputs = model.generate(input_ids=input_sentences, attention_mask=attention_masks, num_beams=5)\n",
        "            corrected_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            target_sentences = tokenizer.batch_decode(target_sentences, skip_special_tokens=True)\n",
        "\n",
        "            # Check accuracy\n",
        "            for corrected, target in zip(corrected_sentences, target_sentences):\n",
        "                if corrected.strip() == target.strip():\n",
        "                    num_correct += 1\n",
        "\n",
        "    accuracy = num_correct / total_samples\n",
        "    return accuracy\n",
        "\n",
        "# Move model to appropriate device\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Calculate and print final accuracy\n",
        "accuracy = calculate_accuracy(val_dataset, tokenizer, model, batch_size=2, device=device)  # Small batch size\n",
        "print(\"Final Accuracy on validation set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "e0dhqQF9CAmP",
        "outputId": "6502b639-c5a6-4e25-8a4d-729d42e32575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6024' max='6024' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6024/6024 53:48, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.045300</td>\n",
              "      <td>0.049532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.039500</td>\n",
              "      <td>0.048387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.048946</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='502' max='502' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [502/502 00:30]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.04838728904724121, 'eval_runtime': 30.3284, 'eval_samples_per_second': 33.104, 'eval_steps_per_second': 16.552, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy on validation set: 0.6264940239043825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BiLSTM"
      ],
      "metadata": {
        "id": "lNS9RSmNNvTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Error Annotated Corpus.csv')\n",
        "\n",
        "# Remove rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "df['Error word & consecutive word'] = df['Error word & consecutive word'].apply(preprocess_text)\n",
        "df['Corrected words & its'] = df['Corrected words & its'].apply(preprocess_text)\n",
        "\n",
        "# Tokenize the sentences\n",
        "all_sentences = df['Error word & consecutive word'].tolist() + df['Corrected words & its'].tolist()\n",
        "all_words = [word for sentence in all_sentences for word in sentence.split()]\n",
        "\n",
        "# Create word-to-index and index-to-word mappings\n",
        "word_counts = Counter(all_words)\n",
        "sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "word_to_index = {word: idx + 1 for idx, word in enumerate(sorted_words)}  # Reserve index 0 for padding\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['Annotation'] = label_encoder.fit_transform(df['Annotation'])\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
        "    df['Error word & consecutive word'].tolist(),\n",
        "    df['Annotation'].tolist(),\n",
        "    test_size=0.1,  # Adjust as needed\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentences, labels, word_to_index):\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.word_to_index = word_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert words to indices\n",
        "        indexed_sentence = [self.word_to_index[word] for word in sentence.split()]\n",
        "\n",
        "        return {\n",
        "            'indexed_sentence': indexed_sentence,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "# Create datasets and dataloaders with padding\n",
        "train_dataset = CustomDataset(train_sentences, train_labels, word_to_index)\n",
        "val_dataset = CustomDataset(val_sentences, val_labels, word_to_index)\n",
        "\n",
        "# Pad sequences\n",
        "train_collate_fn = lambda batch: {\n",
        "    'indexed_sentence': torch.nn.utils.rnn.pad_sequence([torch.tensor(item['indexed_sentence']) for item in batch], batch_first=True),\n",
        "    'label': torch.tensor([item['label'] for item in batch])\n",
        "}\n",
        "\n",
        "val_collate_fn = lambda batch: {\n",
        "    'indexed_sentence': torch.nn.utils.rnn.pad_sequence([torch.tensor(item['indexed_sentence']) for item in batch], batch_first=True),\n",
        "    'label': torch.tensor([item['label'] for item in batch])\n",
        "}\n",
        "\n",
        "# Define the DataLoader with padding\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=train_collate_fn)  # Increased batch size\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, collate_fn=val_collate_fn)  # Increased batch size\n",
        "\n",
        "# Define the BiLSTM model with dropout and regularization\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.5):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.dropout(embedded)  # Applying dropout to the embeddings\n",
        "        output, _ = self.lstm(embedded)\n",
        "        output = torch.relu(self.fc1(output[:, -1, :]))  # Use ReLU activation function for the first fully connected layer\n",
        "        out = self.fc2(output)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model with dropout\n",
        "vocab_size = len(word_to_index) + 1  # Add 1 for padding\n",
        "embedding_dim = 128  # Adjusted embedding dimension\n",
        "hidden_dim = 256  # Adjusted hidden dimension\n",
        "output_dim = len(label_encoder.classes_)\n",
        "dropout = 0.4  # Adjusted dropout rate\n",
        "model = BiLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, dropout=dropout)\n",
        "\n",
        "# Move model to appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Added L2 regularization with weight decay\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30  # Increased number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        inputs, labels = batch['indexed_sentence'].to(device), batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            inputs, labels = batch['indexed_sentence'].to(device), batch['label'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct_preds / total_preds\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss / len(train_dataloader):.4f}, Val Loss: {val_loss / len(val_dataloader):.4f}, Val Acc: {val_accuracy:.2%}\")\n",
        "\n",
        "print('Training finished.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_MxInjY6W_a",
        "outputId": "06b8531e-3de3-4e51-db02-7c3e3a22ad69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Epoch 1/30: 100%|██████████| 71/71 [00:03<00:00, 22.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train Loss: 2.4783, Val Loss: 2.2007, Val Acc: 25.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 71/71 [00:03<00:00, 18.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/30, Train Loss: 2.1468, Val Loss: 2.1946, Val Acc: 30.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 71/71 [00:03<00:00, 22.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/30, Train Loss: 2.0322, Val Loss: 2.0900, Val Acc: 34.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 71/71 [00:03<00:00, 22.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/30, Train Loss: 1.8974, Val Loss: 2.0208, Val Acc: 36.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 71/71 [00:03<00:00, 20.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/30, Train Loss: 1.7821, Val Loss: 1.9217, Val Acc: 40.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|██████████| 71/71 [00:03<00:00, 20.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/30, Train Loss: 1.6330, Val Loss: 1.8602, Val Acc: 40.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|██████████| 71/71 [00:03<00:00, 21.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/30, Train Loss: 1.5018, Val Loss: 1.8399, Val Acc: 43.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|██████████| 71/71 [00:03<00:00, 22.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/30, Train Loss: 1.3750, Val Loss: 1.8284, Val Acc: 42.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|██████████| 71/71 [00:03<00:00, 18.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/30, Train Loss: 1.2337, Val Loss: 1.8349, Val Acc: 44.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|██████████| 71/71 [00:03<00:00, 22.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/30, Train Loss: 1.0949, Val Loss: 1.7993, Val Acc: 48.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|██████████| 71/71 [00:03<00:00, 22.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/30, Train Loss: 0.9632, Val Loss: 1.8873, Val Acc: 47.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|██████████| 71/71 [00:03<00:00, 21.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/30, Train Loss: 0.8789, Val Loss: 1.8581, Val Acc: 51.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|██████████| 71/71 [00:03<00:00, 19.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/30, Train Loss: 0.7687, Val Loss: 2.1101, Val Acc: 50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|██████████| 71/71 [00:03<00:00, 21.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/30, Train Loss: 0.6852, Val Loss: 2.0493, Val Acc: 51.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30: 100%|██████████| 71/71 [00:03<00:00, 19.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/30, Train Loss: 0.5855, Val Loss: 2.1189, Val Acc: 51.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30: 100%|██████████| 71/71 [00:03<00:00, 18.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/30, Train Loss: 0.5340, Val Loss: 2.2054, Val Acc: 51.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30: 100%|██████████| 71/71 [00:03<00:00, 18.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/30, Train Loss: 0.4772, Val Loss: 2.2144, Val Acc: 52.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30: 100%|██████████| 71/71 [00:04<00:00, 14.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/30, Train Loss: 0.4164, Val Loss: 2.4902, Val Acc: 52.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30: 100%|██████████| 71/71 [00:06<00:00, 10.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/30, Train Loss: 0.3613, Val Loss: 2.6054, Val Acc: 53.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30: 100%|██████████| 71/71 [00:09<00:00,  7.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/30, Train Loss: 0.3457, Val Loss: 2.5204, Val Acc: 54.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30: 100%|██████████| 71/71 [00:12<00:00,  5.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/30, Train Loss: 0.3022, Val Loss: 2.6184, Val Acc: 53.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30: 100%|██████████| 71/71 [00:18<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/30, Train Loss: 0.2845, Val Loss: 2.6524, Val Acc: 56.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30: 100%|██████████| 71/71 [00:24<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/30, Train Loss: 0.2589, Val Loss: 3.0733, Val Acc: 54.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30: 100%|██████████| 71/71 [00:25<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/30, Train Loss: 0.2536, Val Loss: 3.1325, Val Acc: 54.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30: 100%|██████████| 71/71 [00:24<00:00,  2.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/30, Train Loss: 0.2084, Val Loss: 2.9564, Val Acc: 54.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/30: 100%|██████████| 71/71 [00:24<00:00,  2.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/30, Train Loss: 0.2020, Val Loss: 3.3480, Val Acc: 54.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/30: 100%|██████████| 71/71 [00:26<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/30, Train Loss: 0.2012, Val Loss: 3.4419, Val Acc: 53.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/30: 100%|██████████| 71/71 [00:26<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/30, Train Loss: 0.1778, Val Loss: 3.3694, Val Acc: 55.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/30: 100%|██████████| 71/71 [00:25<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/30, Train Loss: 0.1806, Val Loss: 3.2710, Val Acc: 56.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/30: 100%|██████████| 71/71 [00:25<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/30, Train Loss: 0.1766, Val Loss: 3.2101, Val Acc: 55.78%\n",
            "Training finished.\n"
          ]
        }
      ]
    }
  ]
}